import os
import sys

# 1. å¼ºåˆ¶æ¸…ç†ä»£ç†ï¼Œé¿å…å¹²æ‰° localhost
for key in ["http_proxy", "https_proxy", "HTTP_PROXY", "HTTPS_PROXY", "all_proxy", "ALL_PROXY"]:
    if key in os.environ:
        del os.environ[key]

# 2. åŸºæœ¬é…ç½®
os.environ["OPENAI_API_KEY"] = "lm-studio"
os.environ["OPENAI_API_BASE"] = "http://127.0.0.1:1234/v1"
os.environ["OPENAI_MODEL_NAME"] = "deepseek-r1-distill-llama-8b"

try:
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage, SystemMessage
except ImportError:
    print("âŒ ç¼ºå°‘ langchain_openai æˆ–ç›¸å…³ä¾èµ–ï¼Œè¯·å…ˆ pip install")
    sys.exit(1)

print(f"ğŸš€ [Verification-Final] è¿æ¥æœ¬åœ° LLM: {os.environ['OPENAI_API_BASE']}")
print(f"ğŸ¤– æ¨¡å‹: {os.environ['OPENAI_MODEL_NAME']}")

# 3. åˆå§‹åŒ– LLM - æœ€çº¯å‡€é…ç½®ï¼ˆä¸å¸¦ toolsï¼‰
llm = ChatOpenAI(
    model=os.environ["OPENAI_MODEL_NAME"],
    openai_api_key=os.environ["OPENAI_API_KEY"],
    openai_api_base=os.environ["OPENAI_API_BASE"],
    temperature=0.1,
    streaming=True,
    request_timeout=60,
    max_retries=0
)

# 4. å‘èµ·çº¯å¯¹è¯è¯·æ±‚
query = "Explain what is SSTI vulnerability briefly."
print(f"\nğŸ—£ï¸ ç”¨æˆ·æŒ‡ä»¤: {query}")
print("Wait for response (Pure Chat)...")

try:
    # æµå¼è¾“å‡º
    full_content = ""
    for chunk in llm.stream([
        SystemMessage(content="You are a security expert. Answer in English."),
        HumanMessage(content=query)
    ]):
        if chunk.content:
            print(chunk.content, end="", flush=True)
            full_content += chunk.content
            
    print("\n\nâœ… æµå¼æ¥æ”¶å®Œæˆã€‚")
    print(f"Total Length: {len(full_content)}")

except Exception as e:
    print(f"\nâŒ è¯·æ±‚å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()

print("\nâœ… éªŒè¯ç»“æŸ")
