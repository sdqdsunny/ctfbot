# CTF-ASAS v2.0 原子实施计划 (ReAct + Platform Adaptation)

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** 将 ASAS Agent 核心架构从关键词路由状态机升级为 ReAct 动态推理模式，并实现与 CTFd 平台的自动化集成。

**Architecture:**

- **Agent Core**: 基于 `LangGraph` 的 ReAct Loop (`agent` -> `tools` -> `agent`)。
- **LLM**: Claude 3.5 Sonnet Tool Use API (via `langchain-anthropic` adaptation)。
- **Platform Layer**: 新增 `asas_mcp.tools.platform` 模块，通过 `requests` 与 CTFd API 交互。

**Tech Stack:** Python 3.10+, LangGraph 0.2+, MCP 1.26+, Anthropic SDK

---

## Phase 1: ReAct 架构迁移 (Infrastructure)

### Task 1: 升级 AgentState 定义

**Files:**

- Modify: `src/asas_agent/graph/state.py:1-17`

**Step 1: Write the failing test**
Create `tests/agent/test_state_v2.py`:

```python
from asas_agent.graph.state import AgentState
from langgraph.graph import MessagesState

def test_agent_state_inheritance():
    # Verify AgentState inherits from MessagesState (or compatible structure)
    # v2 state should primarily use 'messages'
    state = AgentState(messages=[], platform_url="http://test.com")
    assert "messages" in state
    assert state["platform_url"] == "http://test.com"
```

**Step 2: Run test to verify it fails**
Run: `pytest tests/agent/test_state_v2.py -v`
Expected: FAIL (v1 state does not have `messages` key by default or inheritance)

**Step 3: Write minimal implementation**

```python
from langgraph.graph import MessagesState
from typing import Optional

class AgentState(MessagesState):
    """v2.0 ReAct Agent State"""
    platform_url: Optional[str]
    platform_token: Optional[str]
    challenge_id: Optional[str]
```

**Step 4: Run test to verify it passes**
Run: `pytest tests/agent/test_state_v2.py -v`

**Step 5: Commit**
`git add src/asas_agent/graph/state.py tests/agent/test_state_v2.py`
`git commit -m "refactor(agent): upgrade AgentState to inherit from MessagesState for ReAct"`

---

### Task 2: 实现 MCP 到 LangChain Tool 的转换器

**Files:**

- Create: `src/asas_agent/llm/tool_adapter.py`
- Test: `tests/agent/test_tool_adapter.py`

**Step 1: Write the failing test**

```python
import pytest
from asas_agent.llm.tool_adapter import convert_mcp_to_langchain_tools
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_convert_mcp_tools():
    # Mock MCP Client
    mock_client = AsyncMock()
    mock_client.list_tools.return_value = [
        type("Tool", (), {
            "name": "test_tool",
            "description": "A test tool",
            "inputSchema": {
                "type": "object",
                "properties": {"arg1": {"type": "string"}},
                "required": ["arg1"]
            }
        })()
    ]
    
    tools = await convert_mcp_to_langchain_tools(mock_client)
    assert len(tools) == 1
    assert tools[0].name == "test_tool"
    assert tools[0].description == "A test tool"
    # Verify callable
    mock_client.call_tool.return_value = "Success"
    result = tools[0].invoke({"arg1": "value"})
    assert result == "Success"
    mock_client.call_tool.assert_called_with("test_tool", {"arg1": "value"})
```

**Step 2: Run test to verify it fails**
Run: `pytest tests/agent/test_tool_adapter.py -v`

**Step 3: Write minimal implementation**

```python
from langchain_core.tools import StructuredTool
from ..mcp_client.client import MCPToolClient
import asyncio

async def convert_mcp_to_langchain_tools(mcp_client: MCPToolClient):
    mcp_tools = await mcp_client.list_tools()
    langchain_tools = []
    
    for tool in mcp_tools:
        async def _wrapper(**kwargs):
            return await mcp_client.call_tool(tool.name, kwargs)
            
        # Create synchronous wrapper for LangChain if needed, or use async directly
        # For simplicity in this plan, we define the structure. 
        # Note: LangGraph invocations are often async.
        
        lc_tool = StructuredTool.from_function(
            func=None,
            coroutine=_wrapper,
            name=tool.name,
            description=tool.description,
            args_schema=None # We could perform schema conversion here if strict validation needed
        )
        langchain_tools.append(lc_tool)
        
    return langchain_tools
```

**Step 4: Run test to verify it passes**
Run: `pytest tests/agent/test_tool_adapter.py -v`

**Step 5: Commit**
`git add src/asas_agent/llm/tool_adapter.py tests/agent/test_tool_adapter.py`
`git commit -m "feat(agent): add adapter to convert MCP tools to LangChain tools"`

---

### Task 3: 重写 Workflow 为 ReAct Loop

**Files:**

- Modify: `src/asas_agent/graph/workflow.py`
- Test: `tests/agent/test_react_workflow.py`

**Step 1: Write the failing test**

```python
import pytest
from asas_agent.graph.workflow import create_react_agent_graph
from unittest.mock import MagicMock

def test_react_graph_structure():
    mock_llm = MagicMock()
    mock_tools = []
    graph = create_react_agent_graph(mock_llm, mock_tools)
    
    # Check graph nodes
    assert "agent" in graph.nodes
    assert "tools" in graph.nodes
    # Check edges logic (simplified check)
    assert graph.get_entry_point() == "agent"
```

**Step 2: Run test to verify it fails**
Run: `pytest tests/agent/test_react_workflow.py -v`

**Step 3: Write minimal implementation**

```python
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from .state import AgentState

def create_react_agent_graph(llm, tools):
    # Bind tools to LLM
    llm_with_tools = llm.bind_tools(tools)
    
    def agent_node(state: AgentState):
        result = llm_with_tools.invoke(state["messages"])
        return {"messages": [result]}
        
    workflow = StateGraph(AgentState)
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", ToolNode(tools))
    
    workflow.add_edge(START, "agent")
    
    def should_continue(state: AgentState):
        messages = state["messages"]
        last_message = messages[-1]
        if last_message.tool_calls:
            return "tools"
        return END
        
    workflow.add_conditional_edges("agent", should_continue, ["tools", END])
    workflow.add_edge("tools", "agent")
    
    return workflow.compile()
```

**Step 4: Run test to verify it passes**
Run: `pytest tests/agent/test_react_workflow.py -v`

**Step 5: Commit**
`git add src/asas_agent/graph/workflow.py tests/agent/test_react_workflow.py`
`git commit -m "refactor(agent): replace linear graph with ReAct loop workflow"`

---

## Phase 2: 平台适配器 (Platform Adaptor)

### Task 4: 实现 Platform Tools (Requests Layer)

**Files:**

- Create: `src/asas_mcp/tools/platform.py`
- Modify: `src/asas_mcp/server.py` (Register tools)
- Test: `tests/mcp/test_platform_tools.py`

**Step 1: Write the failing test**

```python
import pytest
from unittest.mock import patch, Mock
from asas_mcp.tools.platform import platform_get_challenge

@patch("requests.get")
def test_platform_get_challenge(mock_get):
    # Mock CTFd API response
    mock_response = Mock()
    mock_response.json.return_value = {
        "data": {
            "name": "SQLi 101",
            "description": "Can you bypass the login?",
            "category": "Web"
        }
    }
    mock_get.return_value = mock_response
    
    result = platform_get_challenge("http://ctf.com/challenges/1", "token123")
    assert "SQLi 101" in result
    assert "Can you bypass the login?" in result
```

**Step 2: Run test to verify it fails**
Run: `pytest tests/mcp/test_platform_tools.py -v`

**Step 3: Write minimal implementation**

```python
import requests
import re

def platform_get_challenge(url: str, token: str = None) -> str:
    # 1. Extract ID from URL (simplified regex)
    # Assumes /challenges/{id} or similar
    # For MVP, we'll just fetch the URL content directly if it's an API, 
    # or implement basic CTFd logic
    
    headers = {"Authorization": f"Token {token}"} if token else {}
    
    # Real implementation would be more robust. 
    # Here we mock the concept:
    try:
        # Assuming URL points to API for now, or we scrape
        resp = requests.get(url, headers=headers)
        if resp.status_code == 200:
            data = resp.json().get("data", {})
            return f"题目: {data.get('name')}\n描述: {data.get('description')}\n"
        return f"Error fetching challenge: {resp.status_code}"
    except Exception as e:
        return f"Exception: {str(e)}"

def platform_submit_flag(challenge_id: str, flag: str, base_url: str, token: str) -> str:
    # MVP submission logic
    pass 
```

**Step 4: Run test to verify it passes**
Run: `pytest tests/mcp/test_platform_tools.py -v`

**Step 5: Commit**
`git add src/asas_mcp/tools/platform.py tests/mcp/test_platform_tools.py`
`git commit -m "feat(mcp): add platform interaction tools"`

### Task 5: 注册新工具并集成测试

**Files:**

- Modify: `src/asas_mcp/server.py`
- Test: `tests/agent/test_integration_v2.py`

**Step 1: Register tools**
Modify `src/asas_mcp/server.py` to import and `@mcp.tool` register `platform_get_challenge` and `platform_submit_flag`.

**Step 2: Write integration test**

```python
import pytest
from asas_agent.graph.workflow import create_react_agent_graph
from asas_agent.llm.tool_adapter import convert_mcp_to_langchain_tools
from asas_agent.mcp_client.client import MCPToolClient
from unittest.mock import MagicMock

@pytest.mark.asyncio
async def test_v2_react_loop_mock():
    # This tests the whole chain: LLM -> Agent -> MCP Client -> Tool -> LLM
    # We need a robust Mock LLM that returns tool calls structure
    pass
    # (Detailed mock setup omitted for brevity in plan, but required in execution)
```

**Step 3: Commit**
`git add src/asas_mcp/server.py`
`git commit -m "feat(server): register platform tools and verify integration"`
